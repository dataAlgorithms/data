1. RDD Basics
a. Creating an RDD of strings with textFile() in Python
lines = sc.textFile("README.md")
b. Calling the filter() transformation
pythonLines = lines.filter(lambda line: "Python" in line)
c. Calling the first() action
pythonLines.first()
pythonLines.count()
d. Persisting an RDD in memory
pythonLines.persist # cache() is the same as calling persist() with the default storage level

In summarize, every Spark program and shell session will work as follow
e1: Create some input RDDs from external data
e2: Transfrom them to define new RDDs using transformations like filter()
e3: Ask Spark to persist() any intermediate RDDs that will need to be reused
e4: Launch actions such as count() and first() to kick off a parallel computation,
    which is then optimized and executed by Spark
    
2. RDD Creating
Two way to Creating RDDS
a. Take an existing collection and pass to parallelize
//Python
lines = sc.parallelize(["pandas", "i like pandas"])
//Scala
val lines = sc.parallelize(List("pandas", "i like pandas"))
//Java
JavaRDD<String> lines = sc.parallelize(Arrays.asList("pandas", "i like pandas"));

b. Load data from external storage, eg. textFile()
//Python
lines = sc.textFile("e:\\spark\\README.md") 
//Scala
val lines = sc.textFile("e:\\spark\\README.md")
//Java
JavaRDD<String> lines = sc.textFile("e:\\spark\\README.md");

3. RDD Operations (transformations and actions)
Transformations are operations on RDD that return new RDD, such as map() and filter()
Actions are operatioins that return a result to the driver program or write to storage
 and kick off a computation, such as count() and first()

Transformation return RDD, actions return some other data type

a. Transformation
//Python
inputRDD = sc.textFile("e:\\spark\\log.txt")
errorsRDD = inputRDD.filter(lambda x: "error" in x)
warnsRDD = inputRDD.filter(lambda x: "warn" in x)
badLinesRDD = errorsRDD.union(warnsRDD)
//Scala
val inputRDD = sc.textFile("e:\\spark\\log.txt")
val errorsRDD = inputRDD.filter(line => line.contains("error"))
val warnsRDD = inputRDD.filter(line => line.contains("warn"))
val badLinesRDD = errorsRDD.union(warnsRDD)

b. Action
//Python
print "Input had " + "%s" % badLinesRDD.count() + " concerning lines"
print "Here are 2 examples:"
for line in badLinesRDD.take(2):
    print line
//Scala
println("Input had " + badLinesRDD.count() + " concerning lines")
println("Here are 2 examples:")
badLinesRDD.take(2).foreach(println)

4. Passing function
Passing Functions to Spark
//Python
a. Passing entire functions 
word = rdd.filter(lambda s: "error" in s)

def contiansError(s):
    return "error" in s
word = rdd.filter(containsError)

b. Passing a function with field references (donot do this)
class SearchFunctions:
    def __init__(self, query):
        self.query = query
    def isMatch(self, s):
        return self.query in s
    def getMatchesFunctionReference(self, rdd):
        # Problem: references all of self in self.isMatch
        return rdd.filter(self.isMatch)
    def getMatchesMemberReference(self, rdd):
        # Problem: references all of self in self.query
        return rdd.filter(lambda x: self.query in x)

c. Passing without field references
class WordFunctions:

    ...
    def getMatchesNoReference(self, rdd):
        # Safe: extract only the field we need into a local variable
        query = self.query
        return rdd.filter(lambda x: query in x)

//Scala
class SearchFunctions(val query: String) {
    def isMatch(s: String): Boolean = {
        s.contains(query)
    }

    def getMatchesFunctionReference(rdd: RDD[String]): RDD[String] = {
        //Problem: isMatch means this.isMatch, so we pass all of this
        rdd.map(isMatch)
    }

    def getMatchesFieldReference(rdd: RDD[String]): RDD[String] = {
        //Problem: query means this.query, so we pass all of this
        rdd.map(x => x.split(query))
    }

    def getMatchesNoReference(rdd: RDD[String]): RDD[String] = {
        //Safe: extract just the field we need into a local variable
        val query_ = this.query
        rdd.map(x => x.split(query_))
    }
}

5. Common Transformations and Actions
Element-wise transformations
  inputRDD{1,2,3,4}   
    map x=> x*x Mapped RDD {1,4,9,16}
    filter x => x!= 1 Filterd RDD {2,3,4}

//Python
a. Squaring the values in an RDD
nums = sc.parallelize([1, 2, 3, 4])
squared = nums.map(lambda x: x * x).collect()
for num in squared:
    print "%i," % (num),
b. Splitting lines into words
lines = sc.parallelize(["hello world", "hi"])
words = lines.flatMap(lambda line: line.split(" "))
words.first()

//Scala
a. Squaring the values in an RDD
val input = sc.parallelize(List(1, 2, 3, 4))
val result = input.map(x => x * x)
println(result.collect().mkString(","))
b. Spliting lines into multiple words
val lines = sc.parallelize(List("hello world", "hi"))
val words = lines.flatMap(line => line.split(" "))
words.first()

说明：map与flaMap的区别
map()是将函数用于RDD中的每个元素，将返回值构成新的RDD。
flatmap()是将函数应用于RDD中的每个元素，将返回的迭代器的所有内容构成新的RDD,
         这样就得到了一个由各列表中的元素组成的RDD,而不是一个列表组成的RDD。

scala> val rdd = sc.parallelize(List("coffee panda","happy panda","happiest pand
a party"))
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[14] at parallelize
 at <console>:24

scala>

scala> rdd.map(x=>x).collect
res12: Array[String] = Array(coffee panda, happy panda, happiest panda party)

scala> rdd.flatMap(x=>x).collect
res13: Array[Char] = Array(c, o, f, f, e, e,  , p, a, n, d, a, h, a, p, p, y,  ,
 p, a, n, d, a, h, a, p, p, i, e, s, t,  , p, a, n, d, a,  , p, a, r, t, y)

scala> rdd.flatMap(x=>x.split(" ")).collect
res14: Array[String] = Array(coffee, panda, happy, panda, happiest, panda, party
)

Pseudo set operations
In [2]: rdd1 = sc.parallelize(["coffee", "coffee", "panda", "monkey", "tea"])
In [3]: rdd2 = sc.parallelize(["coffee", "money", "kitty"])
a. distinct(去重）
In [3]: rdd1.distinct().collect()
Out[3]: ['coffee', 'tea', 'panda', 'monkey']
b. union(并集）
In [4]: rdd1.union(rdd2).collect()
Out[4]: ['coffee', 'coffee', 'panda', 'monkey', 'tea', 'coffee', 'money', 'kitty']
c. intersection(交集）
In [6]: rdd1.intersection(rdd2).collect()
Out[6]: ['coffee']
d. subtract(差集)
In [7]: rdd1.subtract(rdd2).collect()
Out[7]: ['tea', 'panda', 'monkey']
e. cartesian(笛卡尔积)
In [8]: rdd3 = sc.parallelize([1,2])
In [9]: rdd4 = sc.parallelize([3,4])
In [10]: rdd3.cartesian(rdd4)
In [11]: rdd3.cartesian(rdd4).collect()
Out[11]: [(1, 3), (1, 4), (2, 3), (2, 4)]

Actions
reduce: operates on two elements of the type 
//Python
sum = rdd.reduce(lambda x, y: x + y)
//Scala
val sum = rdd.reduce((x, y) => x + y)

collect: return all elements from the RDD
rdd.collect() 

count: number of elements in the RDD
rdd.count()

countByValue(): number of times each element occurs in the RDD
rdd.countByValue()

take(num): return num elements from the RDD
rdd.take(2)

top(num): return the top num elements the RDD
rdd.top(2)

takeOrderednum)(ordering): return num elements based on provide ordering
rdd.takeOrdered(2)(myOrdering)

takeSample(withReplacement, num, [seed]): return num elments at random
rdd.takeSample(false, 1)

reduce(func): combine the elements of the RDD in parallel
rdd.reduce((x, y) => x + y)

fold(zero)(func): same as reduce but with provided zero value
rdd.fold(0)((x, y) => x + y)

aggregate(zeroValue)(seqOp, combOp): Similar to recduce() but return a different type
rdd.aggregate((0, 0))((x,y)=>
                      (x._1 + y, x._2 + 1), (x, y) =>
                      (x._1 + y._1, x._2 + y._2))

foreach(func): apply the provided function to each element of the RDD
rdd.foreach(func)

Persistence(Caching)

StorageLevel             SpaceUsed         CpuTime              InMemory            OnDisk
memory_only                high               low                  y                 n
memory_only_ser            low                high                 y                 n
memory_and_disk            high              medium               some              some
memory_only_ser            low                high                 y                 n
memory_and_disk            high              medium               some              some
memory_and_disk_ser        low                high                some              some
disk_only                  low                high                 n                y

//Scala
val result = input.map(x => x * x)
reslut.persit(StorageLevel.DISK_ONLY)
println(result.count())
println(result.collect().mkString(",")) 

5. Working with key/value pair
Creating Pair RDDs

//Python
Creating a pair RDD using the first word as the key 
In [11]: lines = sc.textFile(r"e:\spark\README.md")
In [12]: pairs = lines.map(lambda x: (x.split(" ")[0], x))

//Scala
Creating a pair RDD using the first word as the key
scala> val lines = sc.textFile("e:\\spark\\README.md")
lines: org.apache.spark.rdd.RDD[String] = e:\spark\README.md MapPartitionsRDD[1]
 at textFile at <console>:24
scala> val pairs = lines.map(x => (x.split(" ")(0), x))
pairs: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[3] at map a
t <console>:26

Transformations on Pair RDDs (example: {(1,2),(3,4),(3,6)})
FunctionName/Purpose/Example/Result
reduceByKey(func)
  Combine values with the same key
  rdd.reducebyKey((x, y) => x + y)
  {(1,2),(3,10)}

groupByKey()
  Group values with the same key
  rdd.groupByKey()
  {(1,[2]),(3,[4,6])}

combineByKey(createCombiner,mergeValue,mergeCombiners,partitioner)
  Combine values with the same key using a different result type

mapValues(func)
  Apply a function to each value of a pair RDD without changing the key
  rdd.mapValues(x => x+1)
  {(1,3),(3,5),(3,7)}

flatMapValues(func)
  Apply a function that returns an iterator to each value of a pair RDD
  and for each element returned, produce a key/value entry with the old key
  often used for tokenization
  rdd.flatMapValues(x => (x to 5)
  {(1,3),(1,4),(1,5),(3,4),(3,5)}

keys()
  Return an RDD of just the keys
  rdd.keys()
  {1,3,3}

values()
  Return an RDD of just the values
  rdd.values()
  {2,4,6}

sortByKey()
  Return an RDD sorted by the key
  rdd.sortByKey()
  {(1,2),(3,4),(3,6)}

Transformations on two pair RDDs 
  (rdd={(1,2),(3,4),(3,6)} other={(3,9)})

subtractByKey
  Remove elements with a key present in the other RDD
  rdd.subtractByKey(other)
  {(1,2)}

join
  Perform an inner join between two RDDs
  rdd.join(other)
  {(3,(4,9)),(3,(6,9))}

rightOuterJoin
  Perform a join between two RDDs where the 
  key must be present in the first RDD
  rdd.rightOuterJoin(other)
  {(3,(Some(4),9)),(3,(Some(6),9))}

leftOuterJoin
  Perform a join between two RDDs where the 
  key must be present in the other RDD
  rdd.leftOuterJoin(other)
  {(1,(2,None)),(3,(4,Some(9))),(4,(6,Some(9)))}

cogroup
  Group data from both RDDs sharing the same key
  rdd.cogroup(other)
  {(1,([2],[])),(3,([4,6],[9]))}

eg.
Filter out lines longer than 20 characters (on second element)
//Python
result = pairs.filter(lambda keyValue: len(keyValue[1]) < 20)
//Scala
val result = pairs.filter{case (key, value) => value.length < 20}

Aggregation

Per-key average with reduceByKey() and mapValues() 
//Python
rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
//Scala
rdd.mapValues(x => (x, 1)).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2))

key           value
panda           0
pink            3
pirate          3
panda           1
pink            4
       |
       |mapValues 
      \|/
key            value
panda          (0, 1)
pink           (3, 1)
pirate         (3, 1)
panda          (1, 1)
pink           (4, 1)
       |
       | reduceByKey
      \|/
key             value
panda           (1, 2)
pink            (7, 2)
pirate          (3, 1)

Word count
//Python
rdd = sc.textFile("s3://...")
words = rdd.flatMap(lambda x: x.split(" "))
result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)
or
result = rdd.flatMap(x => x.split(" ")).countByValue()

//Scala
val input = sc.textFile("s3://...")
val words = input.flatMap(x => x.split(" "))
val result = words.map(x => (x, 1)).reduceByKey((x, y) => x + y)
or
val result = input.flatMap(x => x.split(" ")).countByValue()

Per-key average using combineByKey() 
//Python
sumCount = nums.combineByKey((lambda x: (x, 1)),
                             (lambda x, y: (x[0] + y, x[1] + 1)),
                             (lambda x, y: (x[0] + y[0], x[1] + y[1])))
sumCount.map(lambda key, xy: (key, xy[0]/xy[1])).collectAsMap()
//Scala
val result = input.combineByKey(
  (v) => (v, 1),
  (acc: (Int, Int), v) => (acc._1 + v, acc._2 + 1),
  (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)
  ).map{case (key, value) => (key, value._1/value._2.toFloat)}
  result.collectAsMap().map(println(_))

Partition1
coffee    1
coffee    2
panda     3

Partition1 trace:
(coffee, 1) -> new key
accumulators[coffee] = createCombiner(1)
(coffee, 2) -> existing key
accumulators[coffee] = mergeValue(accumulators[coffee],2)
(panda, 3) -> new key
accumulators[panda] = createCombiner(3)

Partition2 trace:
(coffee, 9) => new key
accumulators[coffee] = createCombiner(9)
MergePartitions:
mergeCombiners(partition1.accumulators[coffee],
               partition2.accumulators[coffee])

def createCombiner(value):
    (value, 1)

def mergeValue(acc, value):
    (acc[0] + value, acc[1] + 1)

def mergeCombiners(acc1, acc2):
    (acc1[0] + acc2[0], acc1[1] + acc2[1])

reduceByKey with custom parallelism
//Python
data = [("a", 3), ("b", 4), ("a", 1)]
sc.parallelize(data).reduceByKey(lambda x, y: x + y) # Default parallelism
sc.parallelize(data).reduceByKey(lambda x, y: x + y, 10) # Custom parallelism
//Scala
val data = Seq(("a", 3), ("b", 4), ("a", 1))
sc.parallelize(data).reduceByKey((x, y) => x + y) //Default parallelism
sc.parallelize(data).reduceByKey((x, y) => x + y, 10) //Custom parallelism

Custom sort order, sorting integers as if strings
//Python
rdd.sortByKey(ascending=True, numPartitions=None, keyfunc=lambda x: str(x))
//Scala
val input: RDD[(Int, Venue)] = ...
implicit val sortIntegersByString = new Ordering[Int] {
    override def compare(a: Int, b: Int) = a.toString.compare(b.toString)
}
rdd.sortByKey()

Actions on pairs RDD (example: ({(1,2),(3,4),(3,6)}))

countByKey()
 Count the number of elements for each key
 rdd.countByKey()
 {(1,1),(3,2)}

collectAsMap()
 Collect the result as a map to provide easy lookup
 rdd.collectAsMap()
 Map{(1,2), (3,4), (3,6)}

lookup(key)
 Return all values associated with the provided key
 rdd.lookup(3)
 [4,6]

7. Data partition
//Initialization code: we load the user info from a hadoop SequenceFile on HDFS
//This distributes elements of userData by the HDFS block where they are found
//and does not provide Spark with any way of knowing in which partition a particular UserID is located
val sc = new SparkContext(...)
val uerData = sc.sequenceFile[UserID, UserInfo]("hdfs://...").
                   partitionBy(new HashPartitioner(100)).
                   persist()

//Fucntion called periodically to process a logfile of events in the past 5 minutes
//we assume that this is a sequenceFile containing(UserId, LinkedInfo) pairs
def processNewLogs(logFileName: String) {
    val events = sc.sequenceFile[UserID, LinkInfo](logFileName)
    val joined = userData.join(events)//RDD of (UserId, (UserInfo, LinkInfo)) pairs
    val offTopicVisits = joined.filter {
        case (userId, (userInfo, linkInfo)) => //Expand the tuple into its components
          !userInfo.topics.contains(linkInfo.topic)
    }.count()
    println("Number of visits to non-subscripbed topics: " + offTopicVisits)
}

Determining an RDD's Partitioner

val pairs = sc.parallelize(List((1, 1), (2, 2), (3, 3)))
pairs.partitioner

import org.apache.spark.HashPartitioner
val partitioned = pairs.partitionBy(new HashPartitioner(2))
partitioned.partitioner

PageRank 
//Assume that our neighbor list was saved as a Spark objectFile
val links = sc.objectFile[(String, Seq[String])]("links")
              .partionBy(new HashPartitioner(100))
              .persist()

//Initialize each page's rank to 1.0; since we use mapValues, the resulting RDD
//will have the same partitioner as links
val ranks = links.mapValues(v => 1.0)

//Run 10 iterations of PageRank
for (i <- 0 until 10) {
    val contributions = links.join(ranks).flatMap {
        case (pageId, (links, rank)) =>
            links.map(dest => (dest, rank /links.size))
    }
    ranks = contributions.reduceByKey((x, y) => x + y).mapValues(v => 0.15 + 0.85*v)
}

//Write out the final ranks
ranks.saveAsTextFile("ranks")

//Scala custom partitioner
class DomainNamePartitioner(numPairs: Int) extends Partitioner {
    override def numPartitions: Int = numPairs
    override def getPartition(key: Any): Int = {
        val domain = new Java.net.URL(key.toString).getHost()
        val code = (domain.hashCode % numPartitions)
        if (code < 0) {
            code + numPartitions // Make it non-negative
        } else {
            code 
        }
    }

    //Java equals method to let Spark compare our Partitioner objects
    override def equals(other: Any): Boolean = other match {
        case dnp: DomainNamePartitioner =>
            dnp.numPartitions == numPartitions
        case _ =>
            flase
    }
}

//Python custom partitioner
import urlparse

def hash_domain(url):
    return hash(urlparse.urlparse(url).netloc)

rdd.partitionBy(20, hash_domain)  # Create 20 partitions

8. Load and save files
Text file
a. Loading a text file
//Python
input = sc.textFile("file://README.md")
//Scala
val input = sc.textFile("file://README.md")

Average value per file in Scala
val input = sc.wholeTextFiles("file:/home/salesFiles")
val result = input.mapValues{y =>
    val nums = y.split(" ").map(x => x.toDouble)
    nums.sum / nums.size.toDouble
}

b. Saving as a text file in Python
result.saveAsTextFile(outputFile)

Json file
a. Loading unstructured JSON 
//Python
import json
data = input.map(lambda x: json.loads(x))
//Scala
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.databind.DeserializationFeature

...
case class Person(name: String, lovesPandas: Boolean) //Must be a top-level class
...
//Parse it into a specific case class, we use flatMap to handle errors
//by returning an empty list (None) if we encounter an issue and a
//list with one element if everything is ok (Some(_)).
val result = input.flatMap(record => {
    try {
        Some(mapper.readValue(record, classof[Person]))
    } catch {
        case e: Exception => None
    }
})

b. Saving Json 
//Python
(data.filter(lambda x: x['lovesPandas']).map(lambda x: json.dumps(x)).saveAsTextFile(outputFile))
//Scala
result.filter(p => p.lovesPandas).map(mapper.writeValueAsString(_)).saveAsTextFile(outputFile)

CSV file
a. Loading CSV with textFile
//Python
import csv
import StringIO

def loadRecord(line):
    """Parse a CSV line"""
    input = StringIO.StringIO(line)
    reader = csv.DictReader(input, fieldnames=["name", "favouriteAnimal"])
    return reader.next()

input = sc.textFile(inputFile).map(loadRecord)

//Scala
import Java.io.StringReader
import au.com.bytecode.opencsv.CSVReader

val input = sc.textFile(inputFile)
val result = input.map{line =>
    val reader = new CSVReader(new StringReader(line));
    reader.readNext();
}

b. Loading CSV in full 
//Python
def loadRecords(fileNameContents):
    """Load all the records in a given file"""
    input = StringIO.StringIO(fileNameContents[1])
    reader = csv.DictReader(input, fieldnames=["name", "favoriteAnimal"])
    return reader

fullFileData = sc.wholeTextFiles(inputFile).flatMap(loadRecords)


//Scala
case class Person(name: String favoriteAnimal: String)
val input = sc.wholeTextFiles(inputFile)
val result = input.flatMap{case (_, txt) =>
    val reader = new CSVReader(new StringReader(txt));
    reader.readAll().map(x => Person(x(0), x(1))
}

c. Saving CSV 
//Python
def writeRecords(records):
    """Write out CSV lines"""
    output = StringIO.StringIO()
    writer = csv.DictWriter(output, fieldnames=["name", "favoriteAnimal")
    for record in records:
        writer.writerow(record)
pandaLovers.mapPartitions(writeRecords).saveAsTextFile(outputFile)


//Scala
pandaLovers.map(person => List(person.name, person.favoriteAnimal).toArray)
.mapPartitions{people =>
    val stringWriter = new StringWriter();
    val ssvWriter = new CSVWriter(StringWriter);
    csvWriter.writeAll(people.toList)
    Iterator(stringWriter.toString)
}.saveAsTextFile(outFile)

Loading a SequenceFile

//Python
data = sc.sequenceFile(inFile, "org.apache.hadoop.io.Text", "org.apache.hadoop.io.IntWritable")
//Scala
val data = sc.sequenceFile(inFile, classof[Text], classof[IntWritable]).
  map{case (x, y) => (x.toString, y.get())}

Saving a SequenceFile
//Scala
val data = sc.parallelize(List("Panda", 3), ("Kay", 6), ("Snail", 2))
data.saveAsSequenceFile(outputFile)

Loading keyValue TextInputFormat with old-style API 
//Scala
val input = sc.hadoopFile[Text, Text, KeyValueTextInputFormat](inputFile).map{
    case (x, y) => (x.toString, y.toString)
}

Loading LZO-compressed Json with Elephant Bird
//Scala
val input = sc.newAPIHadoopFile(inputFile, classof[LzoJsonInputFormat],
      classof[LongWritable], classof[MapWritable], conf)
//Each MapWritable in input represent a JSON object

Smaple protocol buffer definition
message Venue {
    required int32 id = 1;
    required string name = 2;
    required VenueType type = 3;
    optional string address = 4;

    enum VenueType {
        COFFEESHOP = 0;
        WORKPLACE = 1;
        CLUB = 2;
        ONNOMONOM = 3;
        OTHER = 4;
    }
}

message VenueResponse {
    repeated Venue results = 1;
}

Elephant Bird protocol buffer writeout 
//Scala
val job = new Job()
val conf = job.getConfiguration
LzoProtobufBlockOutputFormat.setClassConf(classof[Places.Venue], conf);
val dnaLounge = Places.Venue.newBuilder()
dnaLounge.setId(1);
dnaLounge.setName("DNA Lounge")
dnaLounge.setType(Places.Venue.VeneueType.CLUB)
val data = sc.parallelize(List(dnaLounge.build()))
val outputData = data.map{ pb =>
    val protoWritable = ProtobufWritable.newInstance(classof[Places.Venue]);
    protoWritable.set(pb)
    (null, protoWritable)
}
outputData.saveAsNewAPIHadoopFile(outputFile, classof[Text],
  classof[ProtobufWritable[Places.Venue]],
  classof[LzoProtobufBlockOutputFormat[ProtobufWriable[Places.Venue]]], conf)

9. Spark SQL

Aache Hive

Creating a HiveContext and selecting data
//Python
from pyspark.sql import HiveContext

hiveCtx = HiveContext(sc)
rows = hiveCtx.sql("SELECT name, age FROM users")
firstRow = rows.first()
print firstRwo.name

//Scala
import org.apache.spark.sql.hive.HiveContext

val hiveCtx = new oarg.apache.spark.sql.hive.HiveContext(sc)
val rows = hiveCtx.sql("SELECT name, age FROM users")
val firstRow = rows.first()
println(firstRow.getString(0)) //Field 0 is the name

Json

Json loading with Spark SQL
//Python
tweets = hiveCtx.jsonFile("tweets.json")
tweets.registerTempTable("tweets")
results = hiveCtx.sql("SELECT user.name, text FROM tweets")

//Scala
val tweets = hiveCtx.jsonFile("tweets.json")
tweets.registerTempTable("tweets")
val results = hiveCtx.sql("SEELCT user.name, text FROM tweets")

Databases

//Java Database Connectivity
def createConnection() = {
    Class.forName("com.mysql.jdbs.Driver").newInstance();
    DriverManager.getConnection("jdbs:mysql://localhost/test?user=holden");
}

def extractValues(r: ResultSet) = {
    (r.getInt(1), r.getString(2))
}

val data = new JdbcRDD(sc,
  createConnection, "SELECT * FROM panda WHERE ? <= id AND id <= ?",
  lowerBound = 1, upperBound = 3, numPartitions = 2, mapRow = extractValues)
println(data.collect().toList)

Cassandra
//Setting the Cassandra property
val conf = new SparkConf(true).set("spark.cassandra.connection.host", "hostname")

//Loading the entire table as an RDD with key/value data
//Implicits that add functions to the SparkContext & RDDs
import com.datastax.spark.connector._

//Read entire table as an RDD. Assume your table tet was created as
//Create table test.kv(keytext primary key, value int);
val data = sc.cassandraTable("test", "kv")
//Print some basic status on the value field
data.map(row => row.getInt("value")).stats()

//Saving to Cassandra 
val rdd = sc.parallelize(List(Seq("moremagic", 1)))
rdd.saveToCassandra("test", "kv", SomeColumns("key", "value"))

HBase

//Scala example of reading from HBase
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableInputFormat

val conf = HBaseConfiguration.create()
conf.set(TableInputFormat.INPUT_TABLE, "tablename")

val rdd = sc.newAPIHadoopRDD(
  conf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result])

Elasticsearch

//Elasticsearch output
val jobConf = new JobConf(sc.hadoopConfiguration)
jobConf.set("mapred.output.format.class", "org.elasticsearch.hadoop.mr.EsOutputFormat")
jobConf.setOutputCommitter(classOf[FileOutputCommitter])
jobConf.set(ConfigurationOptions.ES_RESOURCE_WRITE, "twitter/tweets")
jobConf.set(ConfigurationOptions.ES_NODES, "localhost")
FileOutputFormat.setOutputPath(jobConf, new Path("-"))
output.saveAsHadoopDataset(jobConf)

//Elasticsearch input
def mapWritableToInput(in: MapWritable): Map[String, String] = {
    in.map{case (k, v) => (k.toString, v.toString)}.toMap
}

val jobConf = new JobConf(sc.hadoopConfiguration)
jobConf.set(ConfigurationOptions.ES_RESOURCE_READ, args(1))
jobConf.set(ConfigurationOptions.ES_NODES, args(2))
val currentTweets = sc.hadoopRDD(jobConf,
    classOf[EsInputFormat[Object, MapWriable]], classOf[Object],
    classOf[MapWritable])
//Extract only the map
//Convert the MapWriable[Text, Text] to Map[String, String]
val tweets = currentTweets.map{case (key, value) => mapWriableToInput(value)}

Accumulators

eg. Accumulator empty line count
//Python
file = sc.textFile(inputFile)
# Create Accumulator[Int] initialized to 0
blankLines = sc.accumulator(0)

def extractCallSigns(line):
    global blankLines  # Make global variable accessiable
    if (line == ""):
        blankLines += 1
    return line.split(" ")

callSigns = file.flatMap(extractCallSigns)
callSigns.saveAsTextFile(outputDir + "/callsigns")
print "Blank lines: %d" % blankLines.value

//Scala
val sc = new SparkContext(...)
val file = sc.textFile("file.txt")
val blankLines = sc.accumulator(0) // Create an Accumulator[Int] initialized to 0

val callSigns = file.flatMap(line => {
    if (line == "") {
        blankLines += 1 //Add to the accumulator
    }
    line.split(" ")
})

classSigns.saveAsTextFile("output.txt")
println("Blank lines: " + blankLines.value)

eg. Accmulator error count
//Python
#Create Accumulators for validating call signs
validSignCount = sc.accumulator(0)
invalidSignCount = sc.accumulator(0)

def validateSign(sign):
    global validSignCount, invalidSignCount
    if re.match(r"\A\d?[a-zA-Z]{1,2}\d{1,4}[a-zA-Z]{1,3}\Z", sign):
        validSignCount += 1
        return True
    else:
        invalidSignCount += 1
        return False

# Count the number of times we contacted each call sign
validSigns = callSigns.filter(validateSign)
contactCount = validSigns.map(lambda sign: (sign, 1)).reduceByKey(lambda (x, y): x + y)

# Force evaluation so the counters are populated
contactCount.count()
if invalidSignCount.value < 0.1 * validSignCount.value:
    contactCount.saveAsTextFile(outputDir + "/contactCount")
else:
    print "Too many errors: %d in %d" % (invalidSignCount.value, validSignCount.value)
